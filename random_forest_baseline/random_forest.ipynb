{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import re\n",
    "import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           question1  \\\n",
      "0  What is the step by step guide to invest in sh...   \n",
      "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
      "2  How can I increase the speed of my internet co...   \n",
      "3  Why am I mentally very lonely? How can I solve...   \n",
      "4  Which one dissolve in water quikly sugar, salt...   \n",
      "\n",
      "                                           question2  is_duplicate  \n",
      "0  What is the step by step guide to invest in sh...             0  \n",
      "1  What would happen if the Indian government sto...             0  \n",
      "2  How can Internet speed be increased by hacking...             0  \n",
      "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
      "4            Which fish would survive in salt water?             0  \n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(\"train.csv\")  # Replace with the dataset path\n",
    "\n",
    "# Keep only necessary columns\n",
    "data = data[['question1', 'question2', 'is_duplicate']]\n",
    "\n",
    "# Drop missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Check the dataset\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit the vectorizer on both questions\n",
    "questions = data['question1'].tolist() + data['question2'].tolist()\n",
    "vectorizer.fit(questions)\n",
    "\n",
    "# Compute TF-IDF vectors for question1 and question2\n",
    "q1_vectors = vectorizer.transform(data['question1'])\n",
    "q2_vectors = vectorizer.transform(data['question2'])\n",
    "\n",
    "# Cosine similarity between question pairs\n",
    "cosine_sim = [cosine_similarity(q1, q2)[0][0] for q1, q2 in zip(q1_vectors, q2_vectors)]\n",
    "\n",
    "# Jaccard similarity\n",
    "def jaccard_similarity(q1, q2):\n",
    "    q1_set = set(q1.split())\n",
    "    q2_set = set(q2.split())\n",
    "    intersection = len(q1_set.intersection(q2_set))\n",
    "    union = len(q1_set.union(q2_set))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "jaccard_sim = [jaccard_similarity(q1, q2) for q1, q2 in zip(data['question1'], data['question2'])]\n",
    "\n",
    "# Word overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "UNI_BLEU_WEIGHTS = (1, 0, 0, 0)\n",
    "BI_BLEU_WEIGHTS = (0, 1, 0, 0)\n",
    "BLEU2_WEIGHTS = (0.5, 0.5, 0, 0)\n",
    "\n",
    "def tokenizer(sentence):\n",
    "    \"\"\"Very basic tokenizer: split the sentence by space into a list of tokens.\"\"\"\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "      words.extend(re.split(_WORD_SPLIT, space_separated_fragment))\n",
    "    return [w for w in words if w]\n",
    "\n",
    "\n",
    "\n",
    "def char_ngram_tokenizer(sentence, n):\n",
    "    \"\"\"Character ngram tokenizer: split the sentence into a list of char ngram tokens.\"\"\"\n",
    "    return [sentence[i:i+n] for i in range(len(sentence)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404287it [04:00, 1679.09it/s]\n"
     ]
    }
   ],
   "source": [
    "overlap = []\n",
    "q1_word_count = []\n",
    "q2_word_count = []\n",
    "word_count_diff = []\n",
    "uni_BLEU = []\n",
    "bi_BLEU = []\n",
    "BLEU2 = []\n",
    "char_bigram_overlap = []\n",
    "char_trigram_overlap = []\n",
    "char_4gram_overlap = []\n",
    "\n",
    "for q1, q2 in tqdm.tqdm(zip(data['question1'], data['question2'])):\n",
    "    t1 = tokenizer(q1)\n",
    "    t2 = tokenizer(q2)\n",
    "\n",
    "    q1_word_count.append(len(t1))\n",
    "    q2_word_count.append(len(t2))\n",
    "    word_count_diff.append(abs(len(t1) - len(t2)))\n",
    "    char_bigram_overlap.append(len(set(char_ngram_tokenizer(q1, 2)).intersection(\n",
    "             set(char_ngram_tokenizer(q2, 2)))))\n",
    "    char_trigram_overlap.append(len(set(char_ngram_tokenizer(q1, 3)).intersection(\n",
    "             set(char_ngram_tokenizer(q2, 3)))))\n",
    "    char_4gram_overlap.append(len(set(char_ngram_tokenizer(q1, 4)).intersection(\n",
    "             set(char_ngram_tokenizer(q2, 4)))))\n",
    "\n",
    "    overlap.append(len(set(q1.lower().split()).intersection(\n",
    "             set(q2.lower().split()))))\n",
    "    \n",
    "    s_function = SmoothingFunction()\n",
    "    uni_BLEU.append(sentence_bleu([t2],\n",
    "                         t1,\n",
    "                         weights=UNI_BLEU_WEIGHTS,\n",
    "                         smoothing_function=s_function.method2))\n",
    "    \n",
    "    bi_BLEU.append(sentence_bleu([t2],\n",
    "                         t1,\n",
    "                         weights=BI_BLEU_WEIGHTS,\n",
    "                         smoothing_function=s_function.method2))\n",
    "    \n",
    "    BLEU2.append(sentence_bleu([t2],\n",
    "                         t1,\n",
    "                         weights=BLEU2_WEIGHTS,\n",
    "                         smoothing_function=s_function.method2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = pd.DataFrame({\n",
    "    'cosine_similarity': cosine_sim,\n",
    "    'jaccard_similarity': jaccard_sim,\n",
    "    'word_overlap': overlap,\n",
    "    'q1_word_count': q1_word_count,\n",
    "    'q2_word_count': q2_word_count,\n",
    "    'word_count_diff': word_count_diff,\n",
    "    'char_bigram_overlap': char_bigram_overlap,\n",
    "    'char_trigram_overlap': char_trigram_overlap,\n",
    "    'char_4gram_overlap': char_4gram_overlap,\n",
    "    'uni_BLEU': uni_BLEU,\n",
    "    'bi_BLEU': bi_BLEU,\n",
    "    'BLEU2': BLEU2\n",
    "})\n",
    "\n",
    "labels = data['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: (363858, 12)\n",
      "Test set size: (40429, 12)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Train set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=500, criterion='entropy',\n",
    "                                   max_depth=10, min_samples_leaf=1,\n",
    "                                   max_features=0.4, n_jobs=3)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.728165425808207\n",
      "Precision: 0.607428327826921\n",
      "Recall: 0.7588203967514312\n",
      "F1-Score: 0.6747365928732094\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"q_vectors\", q1_vectors=q1_vectors, q2_vectors=q2_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .npz file\n",
    "loaded = np.load('q_vectors.npz', allow_pickle=True)\n",
    "\n",
    "# Access the arrays by their names\n",
    "print(loaded['q1_vectors'])\n",
    "print(loaded['q2_vectors'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_forest_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to random_forest_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('random_forest_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "print(\"Model saved to random_forest_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_csv(\"features12.csv\", sep=\",\", index=False, header=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
